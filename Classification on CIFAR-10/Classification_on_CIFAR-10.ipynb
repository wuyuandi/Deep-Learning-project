{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_5_student.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "UlFM7ntrlwE1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"CIFAR 10 Part\"\"\"\n",
        "\n",
        "\n",
        "from __future__ import absolute_import\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import six\n",
        "from six.moves import cPickle\n",
        "from six.moves.urllib.error import HTTPError\n",
        "from six.moves.urllib.error import URLError\n",
        "from six.moves.urllib.request import urlopen\n",
        "\n",
        "\n",
        "def get_file(fname,\n",
        "             origin,\n",
        "             untar=False,\n",
        "             md5_hash=None,\n",
        "             file_hash=None,\n",
        "             cache_subdir='datasets',\n",
        "             hash_algorithm='auto',\n",
        "             extract=False,\n",
        "             archive_format='auto',\n",
        "             cache_dir=None):\n",
        "    \n",
        "    if cache_dir is None:\n",
        "        cache_dir = os.path.expanduser(os.path.join('~', '.keras'))\n",
        "    if md5_hash is not None and file_hash is None:\n",
        "        file_hash = md5_hash\n",
        "        hash_algorithm = 'md5'\n",
        "    datadir_base = os.path.expanduser(cache_dir)\n",
        "    if not os.access(datadir_base, os.W_OK):\n",
        "        datadir_base = os.path.join('/tmp', '.keras')\n",
        "    datadir = os.path.join(datadir_base, cache_subdir)\n",
        "    if not os.path.exists(datadir):\n",
        "        os.makedirs(datadir)\n",
        "\n",
        "    if untar:\n",
        "        untar_fpath = os.path.join(datadir, fname)\n",
        "        fpath = untar_fpath + '.tar.gz'\n",
        "    else:\n",
        "        fpath = os.path.join(datadir, fname)\n",
        "\n",
        "    download = False\n",
        "    if os.path.exists(fpath):\n",
        "        # File found; verify integrity if a hash was provided.\n",
        "        if file_hash is not None:\n",
        "            if not validate_file(fpath, file_hash, algorithm=hash_algorithm):\n",
        "                print('A local file was found, but it seems to be '\n",
        "                      'incomplete or outdated because the ' + hash_algorithm +\n",
        "                      ' file hash does not match the original value of ' +\n",
        "                      file_hash + ' so we will re-download the data.')\n",
        "                download = True\n",
        "    else:\n",
        "        download = True\n",
        "\n",
        "    if download:\n",
        "        print('Downloading data from', origin)\n",
        "\n",
        "        class ProgressTracker(object):\n",
        "            # Maintain progbar for the lifetime of download.\n",
        "            # This design was chosen for Python 2.7 compatibility.\n",
        "            progbar = None\n",
        "\n",
        "        def dl_progress(count, block_size, total_size):\n",
        "            if ProgressTracker.progbar is None:\n",
        "                if total_size is -1:\n",
        "                    total_size = None\n",
        "                ProgressTracker.progbar = Progbar(total_size)\n",
        "            else:\n",
        "                ProgressTracker.progbar.update(count * block_size)\n",
        "\n",
        "        error_msg = 'URL fetch failure on {}: {} -- {}'\n",
        "        try:\n",
        "            try:\n",
        "                urlretrieve(origin, fpath, dl_progress)\n",
        "            except URLError as e:\n",
        "                raise Exception(error_msg.format(origin, e.errno, e.reason))\n",
        "            except HTTPError as e:\n",
        "                raise Exception(error_msg.format(origin, e.code, e.msg))\n",
        "        except (Exception, KeyboardInterrupt) as e:\n",
        "            if os.path.exists(fpath):\n",
        "                os.remove(fpath)\n",
        "            raise\n",
        "        ProgressTracker.progbar = None\n",
        "\n",
        "    if untar:\n",
        "        if not os.path.exists(untar_fpath):\n",
        "            _extract_archive(fpath, datadir, archive_format='tar')\n",
        "        return untar_fpath\n",
        "\n",
        "    if extract:\n",
        "        _extract_archive(fpath, datadir, archive_format)\n",
        "\n",
        "    return fpath\n",
        "\n",
        "\n",
        "def load_batch(fpath, label_key='labels'):\n",
        "    \"\"\"Internal utility for parsing CIFAR data.\n",
        "    # Arguments\n",
        "        fpath: path the file to parse.\n",
        "        label_key: key for label data in the retrieve\n",
        "            dictionary.\n",
        "    # Returns\n",
        "        A tuple `(data, labels)`.\n",
        "    \"\"\"\n",
        "    f = open(fpath, 'rb')\n",
        "    if sys.version_info < (3,):\n",
        "        d = cPickle.load(f)\n",
        "    else:\n",
        "        d = cPickle.load(f, encoding='bytes')\n",
        "        # decode utf8\n",
        "        d_decoded = {}\n",
        "        for k, v in d.items():\n",
        "            d_decoded[k.decode('utf8')] = v\n",
        "        d = d_decoded\n",
        "    f.close()\n",
        "    data = d['data']\n",
        "    labels = d[label_key]\n",
        "\n",
        "    data = data.reshape(data.shape[0], 3, 32, 32)\n",
        "    return data, labels\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    \"\"\"Loads CIFAR10 dataset.\n",
        "    # Returns\n",
        "        Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.\n",
        "    \"\"\"\n",
        "    dirname = 'cifar-10-batches-py'\n",
        "    origin = 'http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n",
        "   # path = get_file(dirname, origin=origin, untar=True)\n",
        "\n",
        "    num_train_samples = 50000\n",
        "\n",
        "    cifar10 = tf.keras.datasets.cifar10\n",
        "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "    print(x_train.shape, x_test.shape)\n",
        "    #print(x_train.shape)\n",
        "    y_train = np.reshape(y_train, (len(y_train), 1))\n",
        "    y_test = np.reshape(y_test, (len(y_test), 1))\n",
        "\n",
        "    return (x_train, y_train), (x_test, y_test)\n",
        "\n",
        "\n",
        "class Cifar10(object):\n",
        "  def __init__(self, batch_size=64, one_hot=False, test=False, shuffle=True):\n",
        "    (x_train, y_train), (x_test, y_test) = load_data()\n",
        "    if test:\n",
        "      images = x_test\n",
        "      labels = y_test\n",
        "    else:\n",
        "      images = x_train\n",
        "      labels = y_train\n",
        "    if one_hot:\n",
        "      one_hot_labels = np.zeros((len(labels), 10))\n",
        "      one_hot_labels[np.arange(len(labels)), labels.flatten()] = 1\n",
        "      labels = one_hot_labels\n",
        "    self.shuffle = shuffle\n",
        "    self._images = images\n",
        "    self.images = self._images\n",
        "    self._labels = labels\n",
        "    self.labels = self._labels\n",
        "    self.batch_size = batch_size\n",
        "    self.num_samples = len(self.images)\n",
        "    if self.shuffle:\n",
        "      self.shuffle_samples()\n",
        "    self.next_batch_pointer = 0\n",
        "\n",
        "  def shuffle_samples(self):\n",
        "    image_indices = np.random.permutation(np.arange(self.num_samples))\n",
        "    self.images = self._images[image_indices]\n",
        "    self.labels = self._labels[image_indices]\n",
        "\n",
        "  def get_next_batch(self):\n",
        "    num_samples_left = self.num_samples - self.next_batch_pointer\n",
        "    if num_samples_left >= self.batch_size:\n",
        "      x_batch = self.images[self.next_batch_pointer:self.next_batch_pointer + self.batch_size]\n",
        "      y_batch = self.labels[self.next_batch_pointer:self.next_batch_pointer + self.batch_size]\n",
        "      self.next_batch_pointer += self.batch_size\n",
        "    else:\n",
        "      x_partial_batch_1 = self.images[self.next_batch_pointer:self.num_samples]\n",
        "      y_partial_batch_1 = self.labels[self.next_batch_pointer:self.num_samples]\n",
        "      if self.shuffle:\n",
        "        self.shuffle_samples()\n",
        "      x_partial_batch_2 = self.images[0:self.batch_size - num_samples_left]\n",
        "      y_partial_batch_2 = self.labels[0:self.batch_size - num_samples_left]\n",
        "      x_batch = np.vstack((x_partial_batch_1, x_partial_batch_2))\n",
        "      y_batch = np.vstack((y_partial_batch_1, y_partial_batch_2))\n",
        "      self.next_batch_pointer = self.batch_size - num_samples_left\n",
        "    return x_batch, y_batch\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "reXnWZjMl6Rj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Ops part\"\"\"\n",
        "\n",
        "\n",
        "# Reference: https://github.com/openai/improved-gan/blob/master/imagenet/ops.py\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# TODO: Optionally create utility functions for convolution, fully connected layers here\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZoqeVEw4mwtd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"net part\"\"\"\n",
        "def net(input, is_training, dropout_kept_prob):\n",
        "  # TODO: Write your network architecture here\n",
        "  # Or you can write it inside train() function\n",
        "  # Requirements:\n",
        "  # - At least 5 layers in total\n",
        "  # - At least 1 fully connected and 1 convolutional layer\n",
        "  # - At least one maxpool layers\n",
        "  # - At least one batch norm\n",
        "  # - At least one skip connection\n",
        "  # - Use dropout\n",
        "  raise NotImplementedError\n",
        "\n",
        "def train():\n",
        "  # Always use tf.reset_default_graph() to avoid error\n",
        "  tf.reset_default_graph()\n",
        "  # TODO: Write your training code here\n",
        "  # - Create placeholder for inputs, training boolean, dropout keep probablity\n",
        "  # - Construct your model\n",
        "  # - Create loss and training op\n",
        "  # - Run training\n",
        "  # AS IT WILL TAKE VERY LONG ON CIFAR10 DATASET TO TRAIN\n",
        "  # YOU SHOULD USE tf.train.Saver() TO SAVE YOUR MODEL AFTER TRAINING\n",
        "  # AT TEST TIME, LOAD THE MODEL AND RUN TEST ON THE TEST SET\n",
        "  raise NotImplementedError\n",
        "\n",
        "def test(cifar10_test_images):\n",
        "  # Always use tf.reset_default_graph() to avoid error\n",
        "  tf.reset_default_graph()\n",
        "  # TODO: Write your testing code here\n",
        "  # - Create placeholder for inputs, training boolean, dropout keep probablity\n",
        "  # - Construct your model\n",
        "  # (Above 2 steps should be the same as in train function)\n",
        "  # - Create label prediction tensor\n",
        "  # - Run testing\n",
        "  # DO NOT RUN TRAINING HERE!\n",
        "  # LOAD THE MODEL AND RUN TEST ON THE TEST SET\n",
        "  raise NotImplementedError\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LysENZV9sisx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import timeit\n",
        "from collections import OrderedDict\n",
        "from pprint import pformat\n",
        "\n",
        "min_thres=0.60\n",
        "max_thres=0.70\n",
        "\n",
        "\n",
        "def compute_score(acc, min_thres, max_thres):\n",
        "    if acc <= min_thres:\n",
        "        base_score = 0.0\n",
        "    elif acc >= max_thres:\n",
        "        base_score = 100.0\n",
        "    else:\n",
        "        base_score = float(acc - min_thres) / (max_thres - min_thres) \\\n",
        "            * 100\n",
        "    return base_score\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    TRAIN = True\n",
        "    if TRAIN:\n",
        "        train()\n",
        "    cifar10_test = Cifar10(test=True, shuffle=False, one_hot=False)\n",
        "    cifar10_test_images, cifar10_test_labels = cifar10_test._images, cifar10_test._labels\n",
        "\n",
        "    start = timeit.default_timer()\n",
        "    np.random.seed(0)\n",
        "    predicted_cifar10_test_labels = test(cifar10_test_images)\n",
        "    np.random.seed()\n",
        "    stop = timeit.default_timer()\n",
        "    run_time = stop - start\n",
        "    correct_predict = (cifar10_test_labels.flatten() == predicted_cifar10_test_labels.flatten()).astype(np.int32).sum()\n",
        "    incorrect_predict = len(cifar10_test_labels) - correct_predict\n",
        "    accuracy = float(correct_predict) / len(cifar10_test_labels)\n",
        "    print('Acc: {}. Testing took {}s.'.format(accuracy, stop - start))\n",
        "    \n",
        "    score = compute_score(accuracy, min_thres, max_thres)\n",
        "    result = OrderedDict(correct_predict=correct_predict,\n",
        "                     accuracy=accuracy, score=score,\n",
        "                     run_time=run_time)\n",
        "\n",
        "    with open('result.txt', 'w') as f:\n",
        "        f.writelines(pformat(result, indent=4))\n",
        "    print(pformat(result, indent=4))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2MnfkcDmqqi_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}