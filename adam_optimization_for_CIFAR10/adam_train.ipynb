{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iw2n3q8IBOCt"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import timeit\n",
    "from collections import OrderedDict\n",
    "from pprint import pformat\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def compute_score(acc, min_thres, max_thres):\n",
    "    if acc <= min_thres:\n",
    "        base_score = 0.0\n",
    "    elif acc >= max_thres:\n",
    "        base_score = 100.0\n",
    "    else:\n",
    "        base_score = float(acc - min_thres) / (max_thres - min_thres) \\\n",
    "                     * 100\n",
    "    return base_score\n",
    "\n",
    "\n",
    "def run(algorithm, x_train, y_train, x_valid, y_valid, x_test, y_test):\n",
    "    start = timeit.default_timer()\n",
    "    np.random.seed(0)\n",
    "    predicted_y_test = algorithm(x_train, y_train, x_valid, y_valid, x_test)\n",
    "    np.random.seed()\n",
    "    stop = timeit.default_timer()\n",
    "    run_time = stop - start\n",
    "\n",
    "    y_test = y_test.flatten()\n",
    "    predicted_y_test = np.asarray(predicted_y_test).flatten()\n",
    "\n",
    "    correct_predict = (y_test == predicted_y_test).astype(np.int32).sum()\n",
    "    incorrect_predict = len(y_test) - correct_predict\n",
    "    accuracy = float(correct_predict) / len(y_test)\n",
    "\n",
    "    return (correct_predict, accuracy, run_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q0MUs_NnBPk0"
   },
   "outputs": [],
   "source": [
    "class DatasetIterator:\n",
    "    def __init__(self, x, y, batch_size):\n",
    "        assert len(x) == len(y)\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.b_sz = batch_size\n",
    "        self.b_pt = 0\n",
    "        self.d_sz = len(x)\n",
    "        self.idx = None\n",
    "        self.randomize()\n",
    "\n",
    "    def randomize(self):\n",
    "        self.idx = np.random.permutation(self.d_sz)\n",
    "        self.b_pt = 0\n",
    "\n",
    "    def next_batch(self):\n",
    "        start = self.b_pt\n",
    "        end = self.b_pt + self.b_sz\n",
    "        idx = self.idx[start:end]\n",
    "        x = self.x[idx]\n",
    "        y = self.y[idx]\n",
    "\n",
    "        self.b_pt += self.b_sz\n",
    "        if self.b_pt >= self.d_sz:\n",
    "            self.randomize()\n",
    "\n",
    "        return x, y\n",
    "\n",
    "\n",
    "def one_hot(a, num_classes):\n",
    "    return np.squeeze(np.eye(num_classes)[a.reshape(-1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sDxRQdIdBTA7"
   },
   "outputs": [],
   "source": [
    "def adam_train(x_train, y_train, x_valid, y_valid, x_test):\n",
    "  \n",
    "    # TODO: Make sure you set all the random seed properly at the beginning \n",
    "    # of this function so your code won't\n",
    "    # have different output every time you run\n",
    "    \n",
    "    \n",
    "    x_train = x_train.reshape((len(x_train), -1))\n",
    "    x_valid = x_valid.reshape((len(x_valid), -1))\n",
    "    x_test = x_test.reshape((len(x_test), -1))\n",
    "    y_train = one_hot(y_train, 10)\n",
    "    y_valid = one_hot(y_valid, 10)\n",
    "\n",
    "    n_inputs = 32 * 32 * 3\n",
    "    n_hidden1 = 200\n",
    "    n_hidden2 = 100\n",
    "    n_outputs = 10\n",
    "\n",
    "    X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "    y = tf.placeholder(tf.float32, shape=(None, n_outputs), name=\"y\")\n",
    "\n",
    "    def neuron_layer(X, n_neurons, name, activation=None):\n",
    "        with tf.name_scope(name):\n",
    "            n_inputs = int(X.get_shape()[1])\n",
    "            stddev = 2 / np.sqrt(n_inputs)\n",
    "            init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev)\n",
    "            W = tf.Variable(init, name=\"kernel\")\n",
    "            b = tf.Variable(tf.zeros([1, n_neurons]), name=\"bias\")\n",
    "            Z = tf.matmul(X, W) + b\n",
    "            if activation is not None:\n",
    "                return activation(Z), W, b\n",
    "            else:\n",
    "                return Z, W, b\n",
    "\n",
    "    with tf.name_scope(\"dnn\"):\n",
    "        hidden1, W1, b1 = neuron_layer(X, n_hidden1, name=\"hidden1\", activation=tf.nn.relu)\n",
    "        hidden2, W2, b2 = neuron_layer(hidden1, n_hidden2, name=\"hidden2\", activation=tf.nn.relu)\n",
    "        logits, W3, b3 = neuron_layer(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        yp = tf.nn.softmax(logits)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "\n",
    "    with tf.name_scope(\"backprop\"):\n",
    "        d_logits = yp - y\n",
    "\n",
    "        d_hidden2 = tf.matmul(d_logits, tf.transpose(W3))\n",
    "        d_W3 = tf.matmul(tf.transpose(hidden2), d_logits)\n",
    "        d_b3 = tf.reduce_sum(d_logits, axis=0, keep_dims=True)\n",
    "\n",
    "        d_2 = tf.to_float(tf.greater(tf.matmul(hidden1, W2) + b2, 0)) * d_hidden2\n",
    "        d_hidden1 = tf.matmul(d_2, tf.transpose(W2))\n",
    "        d_W2 = tf.matmul(tf.transpose(hidden1), d_2)\n",
    "        d_b2 = tf.reduce_sum(d_2, axis=0, keep_dims=True)\n",
    "\n",
    "        d_1 = tf.to_float(tf.greater(tf.matmul(X, W1) + b1, 0)) * d_hidden1\n",
    "        d_W1 = tf.matmul(tf.transpose(X), d_1)\n",
    "        d_b1 = tf.reduce_sum(d_1, axis=0, keep_dims=True)\n",
    "\n",
    "    learning_rate = 0.0001  # this is a good learning rate. you can change\n",
    "\n",
    "    update_ops = []   # contains the ops to update auxilary variables like \n",
    "    # beta_power, m and v\n",
    "    training_ops = []  # contains the ops to update variables\n",
    "    eps = 1e-8\n",
    "\n",
    "    # list of params and gradient\n",
    "    Vs = [W1, b1, W2, b2, W3, b3]\n",
    "    dVs = [d_W1, d_b1, d_W2, d_b2, d_W3, d_b3]\n",
    "    # set betas\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "\n",
    "    \n",
    "    # TODO: write all the code to update betas, m, v, compute m_hat, v_hat\n",
    "    # and update all the variables here. \n",
    "    # Add all tensorflow ops to update betas, m,v to updates_ops\n",
    "    # Add all ops to update V to training_ops\n",
    "    \n",
    "    \n",
    "    with tf.name_scope(\"eval\"):\n",
    "        accuracy = 100.0 * tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logits, axis=1), tf.argmax(y, axis=1)),\n",
    "                                                  dtype=tf.float32))\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    n_epochs = 100\n",
    "    batch_size = 200\n",
    "    n_batches = len(x_train) // batch_size\n",
    "\n",
    "    dataset_iterator = DatasetIterator(x_train, y_train, batch_size)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for epoch in range(n_epochs):\n",
    "            \n",
    "            # compute model\n",
    "            for iteration in range(n_batches):\n",
    "                x_batch, y_batch = dataset_iterator.next_batch()\n",
    "                sess.run(update_ops, feed_dict={X: x_batch, y: y_batch})\n",
    "                sess.run(training_ops, feed_dict={X: x_batch, y: y_batch})\n",
    "\n",
    "            acc_train = accuracy.eval(feed_dict={X: x_batch, y: y_batch})\n",
    "            acc_validation = accuracy.eval(feed_dict={X: x_valid, y: y_valid})\n",
    "            print(epoch, \"Training batch accuracy:\", acc_train, \"Validation set accuracy:\", acc_validation)\n",
    "\n",
    "        # Now that the model is trained, it is the test time!\n",
    "        yp_test = sess.run(tf.argmax(logits, axis=1), feed_dict={X: x_test})\n",
    "\n",
    "    return yp_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 666
    },
    "colab_type": "code",
    "id": "omI7Ew83Bc7q",
    "outputId": "645b7a01-498c-44e5-c7cf-2d83b781860f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 9s 0us/step\n",
      "WARNING:tensorflow:From <ipython-input-3-f5361293394f>:37: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-3-f5361293394f>:44: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "0 Training batch accuracy: 22.0 Validation set accuracy: 19.73\n",
      "1 Training batch accuracy: 20.0 Validation set accuracy: 22.09\n",
      "2 Training batch accuracy: 27.000002 Validation set accuracy: 24.130001\n",
      "3 Training batch accuracy: 22.5 Validation set accuracy: 24.07\n",
      "4 Training batch accuracy: 23.0 Validation set accuracy: 25.71\n",
      "5 Training batch accuracy: 28.0 Validation set accuracy: 25.64\n",
      "6 Training batch accuracy: 25.0 Validation set accuracy: 26.01\n",
      "7 Training batch accuracy: 33.0 Validation set accuracy: 27.11\n",
      "8 Training batch accuracy: 29.499998 Validation set accuracy: 26.499998\n",
      "9 Training batch accuracy: 31.5 Validation set accuracy: 26.480001\n",
      "10 Training batch accuracy: 25.5 Validation set accuracy: 25.64\n",
      "11 Training batch accuracy: 28.0 Validation set accuracy: 27.01\n",
      "12 Training batch accuracy: 31.0 Validation set accuracy: 26.81\n",
      "13 Training batch accuracy: 31.0 Validation set accuracy: 27.46\n",
      "14 Training batch accuracy: 29.499998 Validation set accuracy: 26.269999\n",
      "15 Training batch accuracy: 31.5 Validation set accuracy: 26.789999\n",
      "16 Training batch accuracy: 27.000002 Validation set accuracy: 27.27\n",
      "17 Training batch accuracy: 30.5 Validation set accuracy: 29.109999\n",
      "18 Training batch accuracy: 30.5 Validation set accuracy: 28.76\n",
      "19 Training batch accuracy: 35.0 Validation set accuracy: 27.939999\n",
      "20 Training batch accuracy: 30.5 Validation set accuracy: 27.28\n",
      "21 Training batch accuracy: 34.0 Validation set accuracy: 28.39\n",
      "22 Training batch accuracy: 29.499998 Validation set accuracy: 27.669998\n",
      "23 Training batch accuracy: 28.0 Validation set accuracy: 28.16\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    min_thres = 0.24\n",
    "    max_thres = 0.34\n",
    "    cifar10 = tf.keras.datasets.cifar10\n",
    "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "    x_valid, y_valid = x_train[-10000:], y_train[-10000:]\n",
    "    x_train, y_train = x_train[:-10000], y_train[:-10000]\n",
    "\n",
    "    correct_predict, accuracy, run_time = run(adam_train,\n",
    "                                              x_train, y_train, x_valid, y_valid, x_test, y_test)\n",
    "    score = compute_score(accuracy, min_thres, max_thres)\n",
    "\n",
    "    result = OrderedDict(correct_predict=correct_predict,\n",
    "                         accuracy=accuracy, score=score,\n",
    "                         run_time=run_time)\n",
    "    with open('result.txt', 'w') as f:\n",
    "        f.writelines(pformat(result, indent=4))\n",
    "    print(\"\\nResult:\\n\", pformat(result, indent=4))\n",
    "\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment-3-student.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
